---
title: "projectakhir"
author: "Agil Adi Saputro"
date: "2022-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
#reqURL <- "http://api.twitter.com/oath/request_token"
#accessURL <- "https://api.twitter.com/oauth/access_token"
#api_key <- "kzBcdbYNUKqcTSl3a6LfxMF4c"
#api_secret_key <- "3MxWEzw5pOU9vzG8snPNE9a3vrudtjDbcnRpSv8x8AeFrtvRuN"
#access_token <- "2215418534-8ElUP8fWZ0hJD44rZ8Dg19MGj9OWE2dYrWyFTtd"
#access_secret <- "kMddZ24xv0qdo8fXIqQjYmtbXz7rD9pH95qxzwo3f4q7B"

#setup_twitter_oauth(api_key, api_secret_key, access_token, access_secret)
```

```{r}
#Cari tweet tentang topik work-from-home
#persempit jumlah tweet yang diinginkan dan putuskan untuk memasukkan retweet atau tidak.
#mencari dalam bahasa inggris
#ambil data twitter
#kata <- searchTwitter('work from home', lang="en", n = 1000, resultType = "mixed")

#file_kata <- sapply(kata, function(x) x$getText())

#simpan file csv
#pathOutput <- "C:Programing\\R\\finalProject-123190129\\"
#write.csv(file_kata, paste(pathOutput,'wfhtweets.csv', sep = ''))
#str(file_kata)
```

```{r rlib}
library(tm) #text cleaning
library(wordcloud2) #word cloud
library(vroom) #read data csv
library(here) #simple way to find files

setwd("C:/Programing/R/ProjectAkhirDataScience")
```

```{r load dataset}
#d <- vroom(here("C:/Programing/R/ProjectAkhirDataScience/wfhtweets.csv"))
d <- read.csv("wfhtweets.csv")
komen <- d$x
komenc <- Corpus(VectorSource(komen))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
reviewclean <- tm_map(komenc, removeURL)
removeNL <- function(y) gsub("\n", " ", y)
reviewclean <- tm_map(reviewclean, removeNL)
replacecomma <- function(y) gsub(",", "", y)
reviewclean <- tm_map(reviewclean, replacecomma)
removeRT <- function(y) gsub("RT ", "", y)
reviewclean <- tm_map(reviewclean, removeRT)
removetitik2 <- function(y) gsub(":", "", y)
reviewclean <- tm_map(reviewclean, removetitik2)
removetitikkoma <- function(y) gsub(";", " ", y)
reviewclean <- tm_map(reviewclean, removetitikkoma)
removetitik3 <- function(y) gsub("p…", "", y)
reviewclean <- tm_map(reviewclean, removetitik3)
removeamp <- function(y) gsub("&amp;", "", y)
reviewclean <- tm_map(reviewclean, removeamp)
removeUN <- function(z) gsub("@\\w+", "", z)
reviewclean <- tm_map(reviewclean, removeUN)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
reviewclean <- tm_map(reviewclean,remove.all)
reviewclean <- tm_map(reviewclean, removePunctuation)
reviewclean <- tm_map(reviewclean, tolower)
myStopwords = readLines("C:/Programing/R/ProjectAkhirDataScience/stopwords-en.txt")
reviewclean <- tm_map(reviewclean,removeWords,myStopwords)

dataframe<-data.frame(text=unlist(sapply(reviewclean, `[`)), stringsAsFactors=F)
View(dataframe)
write.csv(dataframe,file = "C:/Programing/R/ProjectAkhirDataScience/dataclean.csv")
```

```{r bagi data}
library(e1071) #untuk naive bayes
library(caret) #untuk klasifikasi data
library(syuzhet) #untuk membaca fungsi get_nrc

d<-read.csv("C:/Programing/R/ProjectAkhirDataScience/dataclean.csv",stringsAsFactors = FALSE) 
review <-as.character(d$text) #merubah text menjadi char 

s <- get_nrc_sentiment(review, cl = NULL, language = "english", lowercase = TRUE)

review_combine<-cbind(d$text,s) #klasifikasi data
par(mar=rep(3,4))
a<- barplot(colSums(s),col=rainbow(10), xlab ='emotion', ylab='count',main='Sentiment Analysis')
barplt <- a
```

```{r bagi data2}
#library untuk penggunaan corpus dalam cleaning data
library(tm)
library(RTextTools)
#library yang terdapat sebuah algoritma naivebayes
library(e1071)
library(dplyr)
library(caret)
df<-read.csv("C:/Programing/R/ProjectAkhirDataScience/dataclean.csv",stringsAsFactors = FALSE)
glimpse(df)

#Set the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced.
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)

corpus<-Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan 
corpus.clean<-corpus%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords, c("work", "from", "home"))%>%
    tm_map(removeWords,stopwords(kind="en"))%>%
    tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)

inspect(dtm[1:10,1:20])

df.train<-df[1:50,]
df.test<-df[51:100,]

dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]

corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]

dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)

dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))

#dim(dtm.train.nb)

dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))

dim(dtm.test.nb)
 
convert_count <- function(x){
    y<-ifelse(x>0,1,0)
    y<-factor(y,levels=c(0,1),labels=c("no","yes"))
    y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)


library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))

kalimat2<-read.csv("dataclean.csv",header=TRUE)

#skoring
kata.positif <- scan("positive-words.txt",what="character",comment.char=";")
kata.negatif <- scan("negative-words.txt",what="character",comment.char=";")
score.sentiment = function(kalimat2, kata.positif, kata.negatif,
                           .progress='none')
{
  require(plyr)
  require(stringr)
  scores = laply(kalimat2, function(kalimat, kata.positif,
                                    kata.negatif) {
    kalimat = gsub('[[:punct:]]', '', kalimat)
    kalimat = gsub('[[:cntrl:]]', '', kalimat)
    kalimat = gsub('\\d+', '', kalimat)
    kalimat = tolower(kalimat)
    list.kata = str_split(kalimat, '\\s+')
    kata2 = unlist(list.kata)
    positif.matches = match(kata2, kata.positif)
    negatif.matches = match(kata2, kata.negatif)
    positif.matches = !is.na(positif.matches)
    negatif.matches = !is.na(negatif.matches)
    score = sum(positif.matches) - (sum(negatif.matches))
    return(score)
  }, kata.positif, kata.negatif, .progress=.progress )
  scores.df = data.frame(score=scores, text=kalimat2)
  return(scores.df)}
hasil = score.sentiment(kalimat2$text, kata.positif, kata.negatif)

#mengubah nilai score menjadi sentimen
hasil$klasifikasi<- ifelse(hasil$score<0, "Negatif",ifelse(hasil$score==0,"Netral","Positif"))
hasil$klasifikasi

#menukar urutan baris
data <- hasil[c(3,1,2)]
#View(data)
write.csv(data, file = "datalabel.csv")

```

```{r global}
library(shiny)
library(here)
library(vroom)
library(dplyr)
library(ggplot2)
library(plotly)
library(syuzhet)

#twitter<- vroom(here("C:/Programing/R/ProjectAkhirDataScience/dataclean.csv"))
#tweet<- twitter$text
dataLabel<- read.csv("datalabel.csv")
ui <- fluidPage(
    titlePanel("Analisis sentimen masyarakat terhadap Kebijakan Work From Home"),
        mainPanel(
            
            tabsetPanel(type = "tabs",
                        tabPanel("Bagan", plotOutput("scatterplot")), 
                        # Plot
                        tabPanel("Data", DT::dataTableOutput('tbl1')),
                        # Output Data Dalam Tabel
                        tabPanel("Wordcloud", plotOutput("Wordcloud"))
                        )
        )
    )

# SERVER
server <- function(input, output) {
    
    # Output Data
    output$tbl1 = DT::renderDataTable({
        DT::datatable(dataLabel, options = list(lengthChange = FALSE))
    })

    output$scatterplot <- renderPlot({produk_dataset<-read.csv("C:/Programing/R/ProjectAkhirDataScience/dataclean.csv",stringsAsFactors = FALSE)

review <-as.character(produk_dataset$text)


s<-get_nrc_sentiment(review)

review_combine<-cbind(produk_dataset$text,s)
par(mar=rep(3,4))
barplot(colSums(s),col=rainbow(10),ylab='count',main='Analisis sentimen masyarakat terhadap Kebijakan Work From Home')
    }, height=400)
    output$Wordcloud <- renderPlot({
   set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)

corpus<-Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan 
corpus.clean<-corpus%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords,stopwords(kind="en"))%>%
    tm_map(removeWords, c("work", "from", "home"))%>%
    tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)

inspect(dtm[1:10,1:20])

df.train<-df[1:50,]
df.test<-df[51:100,]

dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]

corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]

dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)

dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))

#dim(dtm.train.nb)

dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))

dim(dtm.test.nb)
 
convert_count <- function(x){
    y<-ifelse(x>0,1,0)
    y<-factor(y,levels=c(0,1),labels=c("no","yes"))
    y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)


library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
  })
}
shinyApp(ui = ui, server = server)
```